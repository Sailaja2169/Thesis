"""
Vulnerability Prediction ML Model
Predicts phishing susceptibility using machine learning
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (classification_report, confusion_matrix, 
                            roc_auc_score, roc_curve, precision_recall_curve)
from imblearn.over_sampling import SMOTE
import joblib
import yaml
from typing import Dict, Tuple, List
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os

# Add src directory to path to import utils
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils.experiment_tracking import get_tracker


class VulnerabilityPredictor:
    """
    Machine Learning model for predicting phishing vulnerability
    """
    
    def __init__(self, config_path: str = 'config/config.yaml'):
        """Initialize predictor with configuration"""
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.models = {}
        self.best_model = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_importance = None
    
    def prepare_data(self, data_path: str) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Prepare and preprocess data for training
        """
        print("Loading and preparing data...")
        df = pd.read_csv(data_path)
        
        # Define target variable (phishing susceptibility)
        # Assuming 'is_correct' column indicates whether user identified phishing
        target_col = 'is_correct'
        df['vulnerable'] = (~df[target_col]).astype(int)  # Inverse: incorrect = vulnerable
        
        # Feature selection
        feature_cols = [
            'difficulty',
            'cognitive_trigger',
            'response_time_seconds',
            # Add more features from survey data when merged
        ]
        
        # Encode categorical variables
        for col in df.select_dtypes(include=['object']).columns:
            if col in feature_cols:
                if col not in self.label_encoders:
                    self.label_encoders[col] = LabelEncoder()
                    df[col] = self.label_encoders[col].fit_transform(df[col])
                else:
                    df[col] = self.label_encoders[col].transform(df[col])
        
        X = df[feature_cols]
        y = df['vulnerable']
        
        print(f"Features shape: {X.shape}")
        print(f"Target distribution: {y.value_counts().to_dict()}")
        
        return X, y
    
    def train_models(self, X_train: pd.DataFrame, y_train: pd.Series) -> Dict:
        """
        Train multiple ML models
        """
        print("\nTraining machine learning models...")
        
        # Handle class imbalance with SMOTE
        smote = SMOTE(random_state=self.config['ml_models']['random_state'])
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
        
        print(f"After SMOTE: {X_train_balanced.shape}")
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train_balanced)
        
        models_config = {
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=self.config['ml_models']['random_state'],
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                random_state=self.config['ml_models']['random_state']
            ),
            'logistic_regression': LogisticRegression(
                max_iter=1000,
                random_state=self.config['ml_models']['random_state']
            ),
            'svm': SVC(
                kernel='rbf',
                probability=True,
                random_state=self.config['ml_models']['random_state']
            )
        }
        
        results = {}
        
        for name, model in models_config.items():
            print(f"\nTraining {name}...")
            
            # Train model
            model.fit(X_train_scaled, y_train_balanced)
            
            # Cross-validation
            cv_scores = cross_val_score(
                model, X_train_scaled, y_train_balanced,
                cv=self.config['ml_models']['cross_validation_folds'],
                scoring='roc_auc'
            )
            
            self.models[name] = model
            
            results[name] = {
                'cv_mean': float(cv_scores.mean()),
                'cv_std': float(cv_scores.std()),
                'model': model
            }
            
            print(f"{name} CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
        
        # Select best model
        best_model_name = max(results, key=lambda k: results[k]['cv_mean'])
        self.best_model = self.models[best_model_name]
        
        print(f"\nBest model: {best_model_name}")
        
        return results
    
    def evaluate_model(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """
        Evaluate the best model on test set
        """
        print("\nEvaluating best model on test set...")
        
        X_test_scaled = self.scaler.transform(X_test)
        
        # Predictions
        y_pred = self.best_model.predict(X_test_scaled)
        y_pred_proba = self.best_model.predict_proba(X_test_scaled)[:, 1]
        
        # Metrics
        cm = confusion_matrix(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        
        # Classification report
        report = classification_report(y_test, y_pred, output_dict=True)
        
        results = {
            'confusion_matrix': cm.tolist(),
            'roc_auc': float(roc_auc),
            'classification_report': report,
            'predictions': y_pred.tolist(),
            'probabilities': y_pred_proba.tolist()
        }
        
        print(f"\nTest Set ROC-AUC: {roc_auc:.4f}")
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))
        
        return results
    
    def get_feature_importance(self, feature_names: list) -> pd.DataFrame:
        """
        Get feature importance from the best model
        """
        if hasattr(self.best_model, 'feature_importances_'):
            importance = self.best_model.feature_importances_
            
            self.feature_importance = pd.DataFrame({
                'feature': feature_names,
                'importance': importance
            }).sort_values('importance', ascending=False)
            
            print("\nTop 10 Most Important Features:")
            print(self.feature_importance.head(10))
            
            return self.feature_importance
        else:
            print("Feature importance not available for this model type")
            return None
    
    def save_model(self, model_path: str = 'models/saved/vulnerability_predictor.joblib'):
        """
        Save trained model and preprocessors
        """
        model_package = {
            'model': self.best_model,
            'scaler': self.scaler,
            'label_encoders': self.label_encoders,
            'feature_importance': self.feature_importance
        }
        
        joblib.dump(model_package, model_path)
        print(f"\nModel saved to {model_path}")
    
    def load_model(self, model_path: str = 'models/saved/vulnerability_predictor.joblib'):
        """
        Load saved model and preprocessors
        """
        model_package = joblib.load(model_path)
        
        self.best_model = model_package['model']
        self.scaler = model_package['scaler']
        self.label_encoders = model_package['label_encoders']
        self.feature_importance = model_package['feature_importance']
        
        print(f"Model loaded from {model_path}")
    
    def predict_vulnerability(self, features: Dict) -> Dict:
        """
        Predict vulnerability for a single individual
        
        Args:
            features: Dictionary of individual's features
        
        Returns:
            Prediction result with probability
        """
        # Convert to DataFrame
        X = pd.DataFrame([features])
        
        # Encode categorical features
        for col, encoder in self.label_encoders.items():
            if col in X.columns:
                X[col] = encoder.transform(X[col])
        
        # Scale features
        X_scaled = self.scaler.transform(X)
        
        # Predict
        prediction = self.best_model.predict(X_scaled)[0]
        probability = self.best_model.predict_proba(X_scaled)[0]
        
        risk_level = "HIGH" if probability[1] > 0.7 else "MEDIUM" if probability[1] > 0.4 else "LOW"
        
        result = {
            'vulnerable': bool(prediction),
            'vulnerability_probability': float(probability[1]),
            'risk_level': risk_level,
            'recommendations': self._generate_recommendations(features, probability[1])
        }
        
        return result
    
    def _generate_recommendations(self, features: Dict, vulnerability_score: float) -> List[str]:
        """
        Generate personalized recommendations based on vulnerability profile
        """
        recommendations = []
        
        if vulnerability_score > 0.7:
            recommendations.append("URGENT: High vulnerability detected. Immediate training recommended.")
            recommendations.append("Enable email filters and security warnings.")
        elif vulnerability_score > 0.4:
            recommendations.append("Moderate risk. Regular training sessions recommended.")
        else:
            recommendations.append("Low risk. Maintain current security awareness.")
        
        # Personalized based on features
        if features.get('response_time_seconds', 60) < 20:
            recommendations.append("Take more time to evaluate emails before acting.")
        
        return recommendations


def main():
    """Main function for training vulnerability predictor"""
    print("=" * 60)
    print("Phishing Vulnerability Prediction Model Training")
    print("=" * 60)
    
    # Get MLflow experiment tracker
    tracker = get_tracker("phishing_vulnerability_models")
    
    # Start MLflow run
    with tracker.start_run(run_name="vulnerability_predictor_training"):
        predictor = VulnerabilityPredictor()
        
        try:
            # Load and prepare data
            X, y = predictor.prepare_data('data/synthetic/experiment_results.csv')
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y,
                test_size=0.2,
                random_state=42,
                stratify=y
            )
            
            print(f"\nTrain set: {X_train.shape}")
            print(f"Test set: {X_test.shape}")
            
            # Log dataset info
            tracker.log_params({
                "data_shape": X.shape,
                "train_size": X_train.shape[0],
                "test_size": X_test.shape[0],
                "feature_count": X.shape[1],
                "class_distribution": str(y.value_counts().to_dict())
            })
            
            # Train models
            training_results = predictor.train_models(X_train, y_train)
            
            # Log model parameters
            for model_name, result in training_results.items():
                model_params = result['model'].get_params()
                for param_name, param_value in model_params.items():
                    tracker.log_params({f"{model_name}_{param_name}": str(param_value)})
                
                # Log cross validation scores
                tracker.log_metrics({
                    f"{model_name}_cv_roc_auc_mean": result['cv_mean'],
                    f"{model_name}_cv_roc_auc_std": result['cv_std']
                })
            
            # Evaluate best model
            evaluation_results = predictor.evaluate_model(X_test, y_test)
            
            # Log evaluation metrics
            tracker.log_metrics({
                "test_roc_auc": evaluation_results['roc_auc'],
                "test_accuracy": evaluation_results['classification_report']['accuracy'],
                "test_precision": evaluation_results['classification_report']['weighted avg']['precision'],
                "test_recall": evaluation_results['classification_report']['weighted avg']['recall'],
                "test_f1": evaluation_results['classification_report']['weighted avg']['f1-score']
            })
            
            # Feature importance
            feature_importance_df = predictor.get_feature_importance(X.columns.tolist())
            
            # Log feature importance
            if feature_importance_df is not None:
                for _, row in feature_importance_df.iterrows():
                    tracker.log_metrics({
                        f"feature_importance_{row['feature']}": row['importance']
                    })
                
                # Create and log feature importance plot
                plt.figure(figsize=(10, 8))
                sns.barplot(x='importance', y='feature', data=feature_importance_df.head(15))
                plt.title('Feature Importance')
                plt.tight_layout()
                tracker.log_figure(plt.gcf(), "feature_importance.png")
                plt.close()
            
            # Save model
            predictor.save_model()
            
            # Log the model to MLflow
            tracker.log_model(predictor.best_model, "vulnerability_predictor")
            
            print("\n" + "=" * 60)
            print("✓ Model training complete!")
            print("Model tracked with MLflow - run ID: " + mlflow.active_run().info.run_id)
            print("=" * 60)
            
        except FileNotFoundError as e:
            print("Error: Data file not found. Please run data collection scripts first.")
            print(f"Details: {e}")
            
            # Log the error
            tracker.set_tags({"status": "failed", "error": str(e)})


if __name__ == "__main__":
    main()
